# 目次
- [ROSの概要](#ROSの概要)  
- [ROSの環境構築](#ROSの環境構築)  
- [ROSの動作確認](#ROSの動作確認)  
- [公開パッケージを用いた画像処理](#公開パッケージを用いた画像処理)

# ROSの概要
## 歴史
- 2010年、アメリカのWillow Garageが開発した。源流はStanfordのAI Lab（STAIR）で、2018年現在はOSRFが管理している。
- ハードを中心としたロボット指向プログラミングだったのが、ソフトを中心としたモジュール指向プログラミングに変わってきた。
    - ハードに依存しないソフトを開発する必要が出てきた。
- ロボットの開発がソフト中心になってきた。
    - 汎用性、再利用性、移植性などが求められるようになってきた。
- オープンソースの研究開発により、どんどん機能が洗練されていく。
    - ただし、アクティブユーザー数に依存する。
    - 短時間＆ローコストで機能を開発できる。
- 2011年頃はRTMとROSがバチバチしていた。
    - RTMの方がWindowsに対応しているなど、システムとしては優れていたが、ユーザー数はROSの方が多かった。
- ROSの基本は「プロセス間通信のためのライブラリー」と「プログラムのビルドシステム」の2つである。
    - OSとアプリの間にあるミドルウェアの位置付けになる。
    - ソフトから見ると、ドライバーやジョブ管理などのOSの一部として機能しているように見える。
- 2018年現在では、圧倒的にROSが有利な状況となっている。

### 補足
- ROSと同じようなソフトウェアとして、YARP（ヤープ）というロボットを制御するための枠組みもある。イタリアで開発された。
- 2019年4月にNVIDIAがIsaac（アイザック）を提供し始めた。
    - AI研究者がロボットを研究しやすくするためのツールボックス

## 利点
- 最先端のツール群を利用できる。
- 分散型システムを構築できる。
    - 機能を最小単位に分割し、再利用性を高める。
    - バグがシステム全体に影響しないという耐故障性も確保しやすい。
- 複数の言語に対応している。
    - 実際にはC++＆Pythonかな？

## 欠点
- デフォルトの設定では、リアルタイム処理ができない。
- 親（roscore）との通信が切れるとダウンするので危ない。
 - トヨタもシステムダウンを想定してHSRを設計している。
- 画像や点群を高速に遣り取りすることも難しい。

## ROS and others

　|
---|
Player|
YARP|
Orocos|
CARMEN|
Orca|
Microsoft Robotics Studio|

[トップへ](#)

# ROSの環境構築
## Ubuntuの用意
- ROSの演習を行うため、Ubuntu 18.04（Bionic Beaver）が使用できるコンピューターを1人1台用意する。
- 可能であれば、VirtualBoxなどの仮想環境でなく、ネイティブ環境にインストールする。（＝マルチブート環境を構築する。）
- 難しい場合は仮想環境にインストールする。本演習はVirtualBoxでも動作しますが、研究レベルのシミュレーションは動作しません。




[トップへ](#)



# 公開パッケージを用いた画像処理
## USBカメラの利用方法
ROSパッケージ「usb_cam」で画像を取得し、別のROSパッケージ「image_view」で画像を表示する。

内蔵カメラが搭載されていないコンピューターの場合は、外付けカメラ（USBカメラ）を接続する。（VirtualBoxを使用している場合は、カメラが使用できるようにDevicesでUSB（Webcams）にチェックを入れる。）

ターミナルで下記のコマンドを実行し、/dev/video0が存在することを確認する。カメラが２つある場合は/dev/video1も存在する。  
```
$ ls /dev/video*
```  
usb_camのデフォルトのピクセルフォーマットはmjpegなので、v4l2-ctlで設定を確認・変更しておく。

ターミナルを3つ利用する。

１つ目  
```
$ roscore
```

２つ目  
```
$ rosrun usb_cam usb_cam_node
```

３つ目  
```
$ rosrun image_view image_view image:=/usb_cam/image_raw
```  
ウィンドウが1つ表示されればOKです。

---  
※Melodicではcv_cameraが正常に動作しなかった。←Segmentation fault (core dumped)  

2つ目  
```
$ rosrun cv_camera cv_camera_node
```  
3つ目  
```
$ rosrun image_view image_view image:=/cv_camera/image_raw
```  
---  
※uvc_cameraはMelodicに対応していないらしい。

## ROSパッケージの利用方法
毎回、ターミナルを複数開くのは面倒なので、ここからはROSコマンド「roslaunch」を使用する。roslaunchコマンドでlaunchファイルを実行すると、複数のROSノードを1つのターミナルで起動することができる。

サンプルプログラムをGitHub上に公開しているので、~/catkin_ws/srcにcloneして使用する。既にclone済みの場合はpullする。各自でGitHubアカウントを作成してください。

```
$ cd ~/catkin_ws/src
$ git clone https://github.com/suzuki-takuo/stl_ros_sample.git
```  
サンプルプログラムをビルドする。  
```
$ cd ~/catkin_ws
$ catkin_make
$ source devel/setup.bash
```  
下記のとおり実行する。launchディレクトリー内のedge_detection.launchを確認すると、4つのROSノードが起動することが分かる。  
```
roslaunch stl_ros_sample edge_detection.launch
```  
ROSパッケージ「opencv_apps」は画像処理用ライブラリーであるOpenCVをROSで気軽に使用できるようにしたもので、この例では同パッケージ内のROSノード「edge_detection（エッジ検出）」を利用している。

2つのウィンドウが表示され、顔の輪郭などに白線が出ていればOKです。

研究では白線に基づいて顔を検出したり認識したりする。

## ROSパッケージの結合方法
画像処理の結果を利用して、亀を動かす。

camera_infoの情報を利用することになるので、カメラのキャリブレーションを行う。→Camera Calibration

試しに実行する。5つのROSノードが起動する。  
```
roslaunch stl_ros_sample turtle_operation.launch
```  
ROSパッケージ「opencv_apps」のROSノード「hough_circles（ハフ変換）」を利用して真円を検出する。そして、鈴木のROSパッケージ「stl_ros_sample」のROSノード「turtle_operation」で、真円の中心位置を亀の操作量へ変換する。

何か円形のものを撮影し、円を検出させる。画像の上の方に円を検出させると前進、下の方に検出させると後進する。また、左の方に円を検出させると左回転、右の方に円を検出させると右回転する。

恐らくカメラの解像度や撮影時の背景によって上手く動作しないので、ROSパラメーターを調整する。

launchディレクトリー内のturtle_operation.launchを開き、下記のvalueを適当に変更し、上書き保存する。（※改めてcatkin_makeする必要はない。）

- **accumulator_threshold**  
円が検出されないときは値を小さく、円が検出されすぎるときは値を大きくする。  
- **scale_linear**  
亀の直進移動が早すぎる場合は値を大きく、遅すぎる場合は値を小さくする。  
- **scale_angular**  
亀の回転移動が早すぎる場合は値を大きく、遅すぎる場合は値を小さくする。

亀は上手く動かせましたか？

## ポイント
重要なポイントは、5つのノードのうち、鈴木は1つのノードしか開発していないということです。ROSはユーザー数が多いため、画像の取得や表示など、多くの人が必要としている機能は既に実装・公開されています。そのため、自分が得意とする部分の開発に注力することができます。

トヨタ自動車の生活支援ロボットHSRも大所帯で開発していますが、愛県大グループでは点群処理用ROSパッケージの開発のみを行っており、その他の部分はトヨタ自動車や他の研究機関が開発してくれています。ROSを利用することで共同研究しやすくなったと言えます。

ROSの演習は以上です。お疲れ様でした。（何か間違いを見つけたら教えてください。）

[トップへ](#)
